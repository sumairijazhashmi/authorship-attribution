# -*- coding: utf-8 -*-
"""24100004_Phase1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e1BOBuNpJdFaCWIpLgJmGEK5teoI3962

# TASK 1

## Get Tweets from Twitter ##

I am using the snscrape library to scrape data from Twitter
https://github.com/JustAnotherArchivist/snscrape
"""

# import libraries
import snscrape.modules.twitter as sntwitter
import pandas as pd
import csv
import os
import re
import string

"""#### Date of Data Collection: 19 November 2022
#### Time of Data Collection: 5:01 PM PKT
"""

# get tweets using the api
tweets = []

for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:LeoVaradkar').get_items()):
    if i>=1000:
        break
    tweets.append([tweet.rawContent])

print(tweets[0:3])

# store tweets in a csv file
path = os.path.dirname(os.path.abspath("24100004_Phase1.ipynb"))
f = open(path + "/LeoVaradkar_task1.csv", "w")
writer = csv.writer(f)

for tweet in tweets:
    writer.writerow(tweet)

"""# TASK 2"""

cleaned = [None] * len(tweets)

emojis = re.compile("["  u"\U0001F600-\U0001F64F"  u"\U0001F300-\U0001F5FF"  u"\U0001F680-\U0001F6FF"   u"\U0001F1E0-\U0001F1FF"   u"\U00002500-\U00002BEF"   u"\U00002702-\U000027B0"  u"\U00002702-\U000027B0"   u"\U000024C2-\U0001F251"   u"\U0001f926-\U0001f937"   u"\U00010000-\U0010ffff"   u"\u2640-\u2642"   u"\u2600-\u2B55"   u"\u200d"   u"\u23cf"   u"\u23e9"   u"\u231a"   u"\ufe0f"   u"\u3030"   "]+", flags=re.UNICODE)

for i in range(len(tweets)):
    cleaned[i] = re.sub("-", " ", tweets[i][0]) # convert - to space
    cleaned[i] = re.sub(",", " ", cleaned[i]) # convert , to space
    cleaned[i] = re.sub("\d", "", cleaned[i]) # remove digits
    cleaned[i] = re.sub("['؛$'–،٫’?؟۔٪\/:\"|!()°.;″′\-]", "", cleaned[i]) # remove punctuation marks
    cleaned[i] = re.sub("http\S+", "", cleaned[i]) # remove links
    cleaned[i] = re.sub("\n", "", cleaned[i]) # remove new line characters
    cleaned[i] = re.sub("<.*?>", " ", cleaned[i]) # remove html characters
    cleaned[i] = emojis.sub(r'', cleaned[i]) # remove emojis
    cleaned[i] = cleaned[i].lower() # convert to lower case
    cleaned[i] = re.sub("&\w+", "", cleaned[i]) # remove any & words
    cleaned[i] = re.sub("\u2066", "", cleaned[i]) # remove any \u words
    cleaned[i] = re.sub("\xa0", "", cleaned[i]) # remove any \x words
    cleaned[i] = re.sub(u"\N{euro sign}", "", cleaned[i]) # remove any \x words
   
# load stop words
f = open(path + "/stop_words.txt", "r")
stopWords = f.read()
stopWords = stopWords.split("\n")

# remove stop words and ''
print(len(cleaned))
for i in range(len(cleaned)):
    cleaned[i] = cleaned[i].split(" ")
    while('' in cleaned[i]):
        cleaned[i].remove('')
    for j in range(len(stopWords)):
        while(stopWords[j] in cleaned[i]):
                cleaned[i].remove(stopWords[j])
 
# convert back to 1d arr
for i in range(len(cleaned)):
    cleaned[i] = " ".join(cleaned[i])

df = pd.DataFrame(cleaned, columns=None)
df.to_csv("LeoVaradkar_task2.csv", index=False, encoding="utf-8", header=None)

"""# TASK 3"""

# train test split
from sklearn.model_selection import train_test_split

X_train, X_test = train_test_split(cleaned, test_size=0.2, shuffle=True)


def createVocabulary(X_train):
    for i in range(len(X_train)):
        X_train[i] = X_train[i].split(" ")
    # create vocabulary of words
    allvocab = []
    # step 1: add all the words in the dataset 
    for i in range(len(X_train)):
        for j in range(len(X_train[i])):
            allvocab.append(X_train[i][j])



    # step 3: remove any duplicates characters
    vocab = []
    for word in allvocab:
        if word not in vocab:
            vocab.append(word)
    
    return vocab

vocab = createVocabulary(X_train)
print(vocab)

print(len(vocab))

# create bag of words
def create_bags(vocab, cleaned):
    X_complete = []
    all_dicts = []
    arr = [0] * len(vocab)

    for i in range(len(cleaned)):
        cleaned_dict = dict(zip(vocab, arr))
                
        all_dicts.append(cleaned_dict)

    for i in range(len(cleaned)):
        cleaned[i] = cleaned[i].split(" ")

    # # go over every word in the vocab
    #     if that word is in your cleaned[i]
    #         vocab[word] += 1

    for i in range(len(cleaned)):
        for w in cleaned[i]:
            if w in vocab:
                all_dicts[i][w] += 1
        # get values
        bag_of_words = list(all_dicts[i].values())
        X_complete.append(bag_of_words)

    # laplace smoothing - increment number of words by one 
    for i in range(len(X_complete)):
        for j in range(len(X_complete[i])):
            X_complete[i][j] += 1
        

    return X_complete 
      
X_complete = create_bags(vocab, cleaned)

x = 0
flag = False
print("X_train BOW:")
for i in range(len(cleaned)):
    for j in range(len(X_train)):
        if cleaned[i] == X_train[j] and x < 10:
            print(X_complete[i])
            x += 1

for i in range(len(X_test)):
    X_test[i] = X_test[i].split(" ")

x = 0
flag = False
print("X_test BOW:")
for i in range(len(cleaned)):
    for j in range(len(X_test)):
        if cleaned[i] == X_test[j] and x < 10:
            print(X_complete[i])
            x += 1

print("10 raw tweets")
print(tweets[0:10])

print("\n10 cleaned tweets")
print(cleaned[0:10])

